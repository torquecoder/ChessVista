{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "Reading testing images\n",
      "Size of:\n",
      "- Training-set:\t\t564\n",
      "- Test-set:\t\t8\n",
      "- Validation-set:\t140\n",
      "Epoch 1 --- Training Accuracy:   0.0%, Validation Accuracy:   0.0%, Validation Loss: 2.521\n",
      "Epoch 2 --- Training Accuracy:  50.0%, Validation Accuracy:  75.0%, Validation Loss: 1.121\n",
      "Epoch 3 --- Training Accuracy:  50.0%, Validation Accuracy: 100.0%, Validation Loss: 0.731\n",
      "Epoch 4 --- Training Accuracy:  50.0%, Validation Accuracy:  75.0%, Validation Loss: 1.067\n",
      "Epoch 5 --- Training Accuracy:  50.0%, Validation Accuracy:  75.0%, Validation Loss: 0.964\n",
      "Epoch 6 --- Training Accuracy:  50.0%, Validation Accuracy:  75.0%, Validation Loss: 0.710\n",
      "Epoch 7 --- Training Accuracy:  50.0%, Validation Accuracy:  75.0%, Validation Loss: 0.706\n",
      "Epoch 8 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.553\n",
      "Epoch 9 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 1.514\n",
      "Epoch 10 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.509\n",
      "Epoch 11 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.012\n",
      "Epoch 12 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.853\n",
      "Epoch 13 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.452\n",
      "Epoch 14 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.206\n",
      "Epoch 15 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 1.040\n",
      "Epoch 16 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.548\n",
      "Epoch 17 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 2.017\n",
      "Epoch 18 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.130\n",
      "Epoch 19 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.042\n",
      "Epoch 20 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.032\n",
      "Epoch 21 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.075\n",
      "Epoch 22 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.070\n",
      "Epoch 23 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.082\n",
      "Epoch 24 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.243\n",
      "Epoch 25 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.246\n",
      "Epoch 26 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.552\n",
      "Epoch 27 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.834\n",
      "Epoch 28 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 29 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 30 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 31 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 32 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 33 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.016\n",
      "Epoch 34 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.273\n",
      "Epoch 35 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.019\n",
      "Epoch 36 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.202\n",
      "Epoch 37 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.063\n",
      "Epoch 38 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 39 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.080\n",
      "Epoch 40 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 41 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 42 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 43 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.015\n",
      "Epoch 44 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.239\n",
      "Epoch 45 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 46 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.011\n",
      "Epoch 47 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 48 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.263\n",
      "Epoch 49 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 50 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.418\n",
      "Epoch 51 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.219\n",
      "Epoch 52 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.549\n",
      "Epoch 53 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 54 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 55 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 56 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.165\n",
      "Epoch 57 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.013\n",
      "Epoch 58 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 59 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.049\n",
      "Epoch 60 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 61 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.807\n",
      "Epoch 62 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.385\n",
      "Epoch 63 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 64 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 65 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 66 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 67 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 68 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 69 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.245\n",
      "Epoch 70 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 71 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.450\n",
      "Epoch 72 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.019\n",
      "Epoch 73 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 74 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 75 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 76 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 77 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.020\n",
      "Epoch 78 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 79 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.144\n",
      "Epoch 80 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 81 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 82 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 83 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.025\n",
      "Epoch 84 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 85 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.106\n",
      "Epoch 86 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.365\n",
      "Epoch 87 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.656\n",
      "Epoch 88 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 90 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 91 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 92 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 93 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 94 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.050\n",
      "Epoch 95 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 96 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.558\n",
      "Epoch 97 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.253\n",
      "Epoch 98 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 99 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 100 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 101 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 102 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 103 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 104 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.050\n",
      "Epoch 105 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 106 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.054\n",
      "Epoch 107 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.025\n",
      "Time elapsed: 0:03:11\n",
      "Accuracy on Test-Set: 96.4% (135 / 140)\n",
      "Example errors:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b21fb2afc58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b21fb2afc58a>\u001b[0m in \u001b[0;36mprint_validation_accuracy\u001b[0;34m(show_example_errors)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_example_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example errors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mplot_example_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b21fb2afc58a>\u001b[0m in \u001b[0;36mplot_example_errors\u001b[0;34m(cls_pred, correct)\u001b[0m\n\u001b[1;32m    365\u001b[0m     plot_images(images = images[0:4],\n\u001b[1;32m    366\u001b[0m                 \u001b[0mcls_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 cls_pred = cls_pred[0:4])\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b21fb2afc58a>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(images, cls_true, cls_pred)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Plot image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# Show true and predicted classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAD8CAYAAAB5Eq2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/M3MzHLm/wOddrWMkzbZXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58d0f52358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import data_helper\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Convolutional layer 1\n",
    "filter_size_1 = 3\n",
    "num_filters_1 = 32\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter_size_2 = 3\n",
    "num_filters_2 = 64\n",
    "\n",
    "# Convolutional layer 3\n",
    "filter_size_3 = 3\n",
    "num_filters_3 = 64\n",
    "\n",
    "# Fully connected layer\n",
    "fc_size = 128   # Number of neurons in fully connected layer\n",
    "\n",
    "# Number of color channels in an image\n",
    "num_channels = 3    #RGB\n",
    "\n",
    "# Image dimensions\n",
    "img_size = 32\n",
    "\n",
    "# Size of image after flattening to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Class info\n",
    "\n",
    "classes = ['black_bishop', 'black_king', 'black_knight', 'black_pawn', 'black_queen', 'black_rook', 'blank', \\\n",
    "'white_bishop', 'white_king', 'white_knight', 'white_pawn', 'white_queen', 'white_rook']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Validation split\n",
    "validation_size = .2\n",
    "\n",
    "# How long to wait after validation loss stops improving before terminating training\n",
    "early_stopping = None  # use None if you don't want to implement early stoping\n",
    "\n",
    "train_path = 'train_data'\n",
    "test_path = 'test_data'\n",
    "\n",
    "data = data_helper.read_training_sets(train_path, img_size, classes, validation_size = validation_size)\n",
    "test_images, test_ids = data_helper.read_testing_set(test_path, img_size, classes)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "\n",
    "\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev = 0.05)) # Standard Deviation = 0.05\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape = [length]))\n",
    "\n",
    "\n",
    "def new_conv_layer(input,              # The previous layer\n",
    "               num_input_channels,     # Number of channels in previous layer\n",
    "               filter_size,            # Width and height of each filter\n",
    "               num_filters,            # Number of filters\n",
    "               use_pooling = True):    # Use 2x2 max-pooling\n",
    "\n",
    "    # Shape of the filter-weights for the convolution\n",
    "    # This format is determined by the TensorFlow API\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape\n",
    "    weights = new_weights(shape = shape)\n",
    "\n",
    "    # Create new biases, one for each filter\n",
    "    biases = new_biases(length = num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution\n",
    "    # Note the strides are set to 1 in all dimensions\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same\n",
    "    layer = tf.nn.conv2d(input = input, filter = weights, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution\n",
    "    # A bias-value is added to each filter-channel\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value = layer,\n",
    "                               ksize = [1, 2, 2, 1],       # Size of max-pooling window (2x2)\n",
    "                               strides = [1, 2, 2, 1],     # stride on a single image (2x2)\n",
    "                               padding = 'SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU)  (Some alien name as said by Siraj Raval :P)\n",
    "    # It calculates max(x, 0) for each input pixel x\n",
    "    # This adds some non-linearity to the formula and allows us to learn more complicated functions\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights\n",
    "\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,             # The previous layer\n",
    "                 num_inputs,        # Number of inputs from previous layer\n",
    "                 num_outputs,       # Number of outputs\n",
    "                 use_relu = True):  # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(length = num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = [None, img_size_flat], name = 'x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape = [None, num_classes], name = 'y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension = 1) # Returns the index with the largest value across axis of a tensor\n",
    "\n",
    "\n",
    "layer_conv1, weights_conv1 = \\\n",
    "new_conv_layer(input = x_image,\n",
    "               num_input_channels = num_channels,\n",
    "               filter_size = filter_size_1,\n",
    "               num_filters = num_filters_1,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "new_conv_layer(input = layer_conv1,\n",
    "               num_input_channels = num_filters_1,\n",
    "               filter_size = filter_size_2,\n",
    "               num_filters = num_filters_2,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "new_conv_layer(input = layer_conv2,\n",
    "               num_input_channels = num_filters_2,\n",
    "               filter_size = filter_size_3,\n",
    "               num_filters = num_filters_3,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = new_fc_layer(input = layer_flat,\n",
    "                         num_inputs = num_features,\n",
    "                         num_outputs = fc_size,\n",
    "                         use_relu = True)\n",
    "\n",
    "layer_fc2 = new_fc_layer(input = layer_fc1,\n",
    "                         num_inputs = fc_size,\n",
    "                         num_outputs = num_classes,\n",
    "                         use_relu = False)\n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2, name = 'y_pred')\n",
    "y_pred_cls = tf.argmax(y_pred, dimension = 1)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = layer_fc2, labels = y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "train_batch_size = batch_size\n",
    "\n",
    "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    acc = session.run(accuracy, feed_dict = feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict = feed_dict_validate)\n",
    "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "\n",
    "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        feed_dict_validate = {x: x_valid_batch, y_true: y_valid_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "\n",
    "\n",
    "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
    "        if i % int(data.train.num_examples/batch_size) == 0:\n",
    "            val_loss = session.run(cost, feed_dict = feed_dict_validate)\n",
    "            epoch = int(i / int(data.train.num_examples / batch_size))\n",
    "\n",
    "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
    "\n",
    "            if early_stopping:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                if patience == early_stopping:\n",
    "                    break\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(\"trained_model\"):\n",
    "        os.makedirs(\"trained_model\")\n",
    "    saver.save(session, 'trained_model/trained_model',global_step = 15000)\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred = None):\n",
    "\n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return\n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "\n",
    "\n",
    "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=5, wspace=5)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.valid.images[incorrect]\n",
    "\n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.valid.cls[incorrect]\n",
    "\n",
    "    # Plot the first 4 images.\n",
    "    plot_images(images = images[0:4],\n",
    "                cls_true = cls_true[0:4],\n",
    "                cls_pred = cls_pred[0:4])\n",
    "\n",
    "def print_validation_accuracy(show_example_errors = False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.valid.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape = num_test, dtype = np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.valid.images[i:j, :].reshape(batch_size, img_size_flat)\n",
    "\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.valid.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images, y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict = feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    cls_true = np.array(data.valid.cls)\n",
    "    cls_pred = np.array([classes[x] for x in cls_pred])\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred = cls_pred, correct = correct)\n",
    "\n",
    "\n",
    "\n",
    "optimize(num_iterations = 15000)\n",
    "print_validation_accuracy(show_example_errors = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
