{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "Reading testing images\n",
      "Size of:\n",
      "- Training-set:\t\t564\n",
      "- Test-set:\t\t8\n",
      "- Validation-set:\t140\n",
      "Epoch 1 --- Training Accuracy:   0.0%, Validation Accuracy:   0.0%, Validation Loss: 2.583\n",
      "Epoch 2 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.925\n",
      "Epoch 3 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 1.238\n",
      "Epoch 4 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 1.013\n",
      "Epoch 5 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.447\n",
      "Epoch 6 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.108\n",
      "Epoch 7 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.607\n",
      "Epoch 8 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.800\n",
      "Epoch 9 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.534\n",
      "Epoch 10 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.037\n",
      "Epoch 11 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.201\n",
      "Epoch 12 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.125\n",
      "Epoch 13 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.074\n",
      "Epoch 14 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.446\n",
      "Epoch 15 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.190\n",
      "Epoch 16 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.012\n",
      "Epoch 17 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.051\n",
      "Epoch 18 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.166\n",
      "Epoch 19 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.476\n",
      "Epoch 20 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 21 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 22 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.005\n",
      "Epoch 23 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 24 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.106\n",
      "Epoch 25 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.151\n",
      "Epoch 26 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 27 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.016\n",
      "Epoch 28 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 2.269\n",
      "Epoch 29 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.008\n",
      "Epoch 30 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.218\n",
      "Epoch 31 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.468\n",
      "Epoch 32 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 33 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 1.491\n",
      "Epoch 34 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 35 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.005\n",
      "Epoch 36 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.735\n",
      "Epoch 37 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.076\n",
      "Epoch 38 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 39 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.090\n",
      "Epoch 40 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.060\n",
      "Epoch 41 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.203\n",
      "Epoch 42 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.088\n",
      "Epoch 43 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 44 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 45 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 46 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 47 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 48 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 49 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.457\n",
      "Epoch 50 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.018\n",
      "Epoch 51 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 52 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 53 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.130\n",
      "Epoch 54 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.554\n",
      "Epoch 55 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 56 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 57 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 58 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 59 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.039\n",
      "Epoch 60 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 61 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 62 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 63 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 1.176\n",
      "Epoch 64 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 65 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.018\n",
      "Epoch 66 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.329\n",
      "Epoch 67 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 68 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 6.896\n",
      "Epoch 69 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 70 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 71 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.577\n",
      "Epoch 72 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.030\n",
      "Epoch 73 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 74 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 75 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 76 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 77 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.032\n",
      "Epoch 78 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.010\n",
      "Epoch 79 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 80 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 81 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 82 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 83 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 84 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.631\n",
      "Epoch 85 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.174\n",
      "Epoch 86 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 87 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 88 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.289\n",
      "Epoch 90 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 91 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 92 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 93 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 94 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.010\n",
      "Epoch 95 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 96 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 97 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 98 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.258\n",
      "Epoch 99 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 100 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 101 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.450\n",
      "Epoch 102 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 103 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%, Validation Loss: 0.747\n",
      "Epoch 104 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 105 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 106 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 1.997\n",
      "Epoch 107 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.026\n",
      "Time elapsed: 0:04:43\n",
      "Accuracy on Test-Set: 96.4% (135 / 140)\n",
      "Example errors:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7585af863b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7585af863b29>\u001b[0m in \u001b[0;36mprint_validation_accuracy\u001b[0;34m(show_example_errors)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_example_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example errors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mplot_example_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7585af863b29>\u001b[0m in \u001b[0;36mplot_example_errors\u001b[0;34m(cls_pred, correct)\u001b[0m\n\u001b[1;32m    365\u001b[0m     plot_images(images = images[0:4],\n\u001b[1;32m    366\u001b[0m                 \u001b[0mcls_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 cls_pred = cls_pred[0:4])\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7585af863b29>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(images, cls_true, cls_pred)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;31m# Plot image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# Show true and predicted classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAD8CAYAAACbzrbdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmUHWWd//H3p5dsnTQhgOwESFgEfyNiIOLozOCwO46I\nGyiOIpFRQMHlCAcRUeNP5Mwo/BTUyHYwbM6giAiCjEgCgUA2IAlCAgwxKJskkEBC093f3x/Pczs3\nndtb0n1vd/XndU6frlu3bj1PVX3r+VY9VbeuIgIzMzMrlrpaV8DMzMz6nxO8mZlZATnBm5mZFZAT\nvJmZWQE5wZuZmRWQE7yZmVkBOcGbmZkVkBO8mZlZATnBm5mZFVBDrSsw3IwdOza22WabLt+PCCR1\nO48VK1a8GBHb9XfdrFh6irXecKxZTxxng5cTfJVts802nHXWWRXfa2xsZNWqVWy77basX7++y3mc\ndtppTw9U/aw4KsXamjVrmDhxIqtWreo4mNx7772ZP38+Y8eO3WQejjXrSXdtWm85zgaGu+gHidbW\nVp566iluuukm2traaG9vr3WVrEAkERHU19czZcoUWlpaaGho4IUXXqC5uZmmpib8uxS2pSKixzhq\na2ujra2N1tbWKtVq+HKCHwTa29sZMWIEd9xxB5K46KKL2NIuL7Nyzz//PIceeihNTU1st912TJ06\nleeff57Pf/7zrFixgpaWFg455BBee+21WlfVhqiXXnqJ+++/n7lz57J27VoA6uvrO/4A6urquPzy\ny7n00kt57rnnfFA5wNxFPwg0NjZyzTXX0NjYSHt7OxMmTGDmzJkcddRRHTuG2ZbYfvvtufHGG5k2\nbRpr165l6tSpHHzwwUiitbWV973vfcyYMYM99tjDja71WUTw8ssvc8MNN1BfX8/kyZPZeuut+epX\nv0pzczOtra2cf/75tLW1MXLkSOrq6nq818i2nBP8IFBXV9fRXVVqXNesWeMdwPpNe3s748ePZ+zY\nsYwdO7YjtiKCvffem0cffZRJkybR1tZW45paEdTV1RERnHfeecCGdq2xsZH6+nra2trcvlWBE/wg\n0Nrayq677sozzzxDRPDGG28wadIkGhsb3eBav2lra+Piiy/m3HPP7Rj3yiuvMGbMGGbNmsVOO+3E\nyJEja1hDG6okscMOO3Duueeydu1abrrpJp588knq6+tpbGykrq6OxsZGAHbbbTfWr1/vnqIq8DX4\nQaCuro5jjjmG1atXs2LFCpYsWcJHPvKRWlfLCmb9+vUceeSRtLW1sW7dOtauXdvRe3T44YfT0tJS\n6yraEDZ69GgmTJhAfX09K1euZMyYMYwdO5ampiaampoYM2YM48ePZ//992fatGnssMMOPosfYE7w\ng0BLSwtz5syhubmZXXbZhbe+9a3ce++9bnCtXzU3N3PnnXdSX1/P6NGjGTduHM3NzYwcOZJHHnmE\ncePG1bqKNsRJQtJGN9eV/hobG2loaGDNmjVAOrGxgeU1PAjU1dXxzDPPAOlaVV1dHUuXLnUXlvW7\ndevWdQyXvtI0atQoXn311RrWyopGUseNdOXDTurV5bU9CDQ3N3dcf5dEW1sbs2fPprm5udZVswKR\ntNHXL0vdo+PHj6exsdHdpdZvKp3B19fXU1dX5yRfRV7Tg0B7e3vHDSil16NGjfINT9av6uvrmTp1\n6kYPUZJEU1MTkyZNcsNr/aJzF31DQwMNDQ0dr30gWT3eo2usra2NJUuWbJTgI4Idd9yRBx54wHfR\nW79Zt24dBx544CYNrCQmT568Ufe92ZYona2XEntdXV1Hsgec5KvECb6GStfb582bt9H1dkmMGTOG\nG2+8seP7pGZbQhJr1qxhxIgRFRvXcePG+dkLtsVKbVXn6+7lD7apr69n4cKF7jGqAn8PvkYigvXr\n1zN//nzWrVu3yZOd6uvrWbVqFddeey2HHnooO+ywg3cI2yylM6mPf/zjXU7T1NTEIYccwvLly5Hk\n30KwPpPE8uXLee211zjooIM6ztrLb7RbsWIFL7zwAk1NTbWu7rDgBF9Do0eP5vHHH+94bnNnY8eO\n5cknn+QDH/iAz6xss0UEa9euZdSoUaxevbpiLDU1NTF+/HjWrFlT8VflzHrjlVde4dVXX+34caPS\nDxyVDho/+MEPMmfOHEaPHl3rqg4LTvA1UroR5VOf+lSX07S0tNDe3s6YMWOqVzErnIigvb2dq6++\nutsDxfb2dhoaGnxJyDZLRDBlypRNLjeWa2trY+rUqQDuJaoCeWeuLkkvAFv628cTI2K7/qiPFZdj\nzarBcTZ4OcGbmZkVkO/aMjMzKyAneDMzswJygjczMyugXiV4SdtIWpT/npX0TNnrEQNRMUmTJS3q\n4r17JB2wGfOcJumiXk47U9KxFcZPlfSDbj53mKSb+lq3vshlvJzX/6OSvraF85su6cx+qFPF5ZZ0\npaR9JNVJOrvTe1WLLUkrJY3vxXQNklZ38d5pkj6ehz8taYf+rGMt5eVuy+t+saQbJG3295l6sy/U\nej8v+8ywa+O2VF/bjVy3F8rarU9vYfkV2+je1LG7dd+Lcjdr29RCr74mFxF/Aw4AkHQ+sDYi/qN8\nGqXvQygiCv3dh4iYC8ytdT2AuyLiWEljgYcl3RIRD5XelNQQEa01rF+HiDgJUp2As4ELyt7brNiS\nVB8RVX+Ob0RcUvby08AC4Nlq12MArYmIA/I6vx74DPD/Sm8WdT8f7m1cFduLayLizHxgvFjSzRHx\nYg3qMSxsURd9PgpaLOknpIZu1/IzH0nHS7osD28v6ZeS5kl6QNI7elFEo6SfS3pE0i8qnU1ImpHn\nuUTSeWXjp0q6T9JDkuZKGtPpc/8q6V5JE7op/0hJsyU9Luno/LmOsxJJ78nzXyRpgaTS45nG5WV9\nTNLVZWUenqd9RNLPSmcG+czygrxe5krasxfrBoCIWEta95PyEfL1km4BbsvzPjvP9+FO6+e8XL/f\nA3v1VI6kOyTtl4cfkXROHv6upE/1sNylI94L8jSLSu9L+mSu3yLgvYDy+M6xtbukyEfkDwDnSLq1\n7GzgKUnz87w+X2k9l9XnG5KWSjpJ0g8l3ZHHHynpqrLpLsjb9z5Jb8rjpks6U9JHSQnhhlzWCEkH\nSbo71+M2Sdt3sz4PytvkPkn/kZd/kzMwSb+T9K48fHSefoHS2XVT2bw2KTev91JcPSbpnT1t55JI\nX6+ZDUyusC127KYu781l3QO8v5fF1Ww/lzQpf+7BvG1L7ddekm4qW/YlwFOkNm5N2fpeJGlmntc7\nJD0n6VVJr0j6cB4/U9LFkuZIelLSBwbJss+U9J+S7gL+r6RtJd2c43KOpLfk6SqO7zSvz0n6raRR\nlcrqLCKeBf4X2C2v958qtUVXKvUkfV8b2q1puYw6SZcq7bu/AbbtTVnA2yTdJWmZKvQa5BiYLWlh\n3qZTy947J2+bhyR9p9Pn6vM6PL9SoXk5Vkv6Qd5Pfi9pm/zeZ3PMPSTpvySNztM/md/fVlJ7aZ/N\n23n3vK4uz/H3pKTTelrRffoDzge+kocnA+3AQfl1A7C6bNrjgcvy8A3AO/Lw7sDiPDwV+EmFciYD\nUfaZq4Ez8/A9wAF5eEJZ2bOB/YBRpJ3xwPzeVkA9MA24CPgQMAvYqpvlnAncQjoI2gf4MzASOAy4\nKU9zGzA1D4/NZRwGrAJ2zK8fBN4BjMnzmJSnvwY4PQ+vBM7Kw58uzb+bupXXYTtgRa7jNNL3UbfO\n7x0DXEpKmnXA74B3AgcDDwGj87p5qmzdngZMq1DmucC/A1vnZbo1j58NTOpqucu3V4X4eAtwE9CQ\nX88nHeGXtn95bI3M8XBcfv1vwNpc9g152U4H9gXeyOOnAss6reddczlz8rh7gQdy3b4NnJyHAzg6\nT/N94Ow8PJ3KcTgSmANsm19/HJjRzTZcAvx9Hv4BsCgPTwMuKpvud8C7gDcBdwNj8vivAed0V26u\n3/fy8L8Cv+shrjq2D9BIiv/PVNgWXdVlTF7Hk0gxdyMb4nSw7ue3Ah/Lw2cAq0lt3E9JsVla9huB\nE/Mytpat7x8Cf8rDzwEfzMMfJPWGQGonluV18ndl09d62WfmZazLr38MfC0PHwHM62H8dODM/Pcr\nYEQP8dUR23nZXwDG5/k8AIzK753Khv1tJLAQ2A34SF6XdcAuwCvAsXm67wDHVChzOumgdBQpblcC\n2+fyS/vcmLKy9wXm5uH35fU9utN2uAeYQmp3zuphfwrgo/n1t8qWf5uy6S4APpeH7yS15ceS2tCz\nSO30E2XLMxsYkZfnb0B9V3XojyfZPRERD/ZiusOAfbThyUZbSxod3Xd5PxUR9+fhmcAppOAtd4Kk\nUqO8Eyn4RwIrImIBQES8DB1PVTqclOCOiHT2251fROqOe0zSn9n0TPde4CJJ1wI3RsTaXMb9EfHX\nXOYi0gHNG8CyiHgif/ZqUjL5UX59Xf5/DWVd2N04VNJCUuPz7Yh4TNK7gTsiYlWe5gjgaNIOAukg\nZG/Ske+NEbEOWJePhoFNuqDLzSat/78Cvwbem88Ydo6IJyTt0cVy39/F/CDFxEHAvLzediHt9CWV\nYutX+f/OwCu57MNIDfO3gc/ndfKX/N7pbLyebwEuBL6pdD1+LbAceBvwbuDnebp1EXFbHp6f3+vO\nm4H9gTvzstSTGpNNSNqW1Gjcm0f9HDi0h/m/kxTbc/L8R5Aamp7K/WXZMuzeQxmQe1jy8N3AVcBE\nNt4WXdVlP+DxUoxLuoZ0INbTpa1a7ueHkBpySNvhmxWmeYINcflm0joure8JqUiNJzW4M0tn9EBD\nPiP/G3BVpBb6YUk7D5JlB/iv2HDJ4V2kXjQi4g5JVyn1zHQ1HuAk0knFcdG7rvWPS/pHoIV0IrE6\n1/nXEbE+T3ME8GZJx+fXW5Ha3n8Arsv1XSnpj6WZRkR39yHdlOe9XtIsUpvzp7L3RwI/kvRW0sHb\npDz+MOCK3E4SES+VfeZy4NqI+F4Py9sK/Fcenglcm4f/TtK3SAc440jtEqR29h9IcfZd0glf533n\nlohoAZ6X9BLpJK/iZcL+SPCvlg23k7tYs/LuGgEH54r1Vuen8Gz0WtJepKPug3OgzMxlqsJnS5aT\njt72YkPi26zyI2K6pJtJwf+gpH/Kb71eNlkbaT339DD5vj5x6K6IqHSDSfn2EDA9Ii4vn0DSVzaj\nvLnAZcBfgN+QkvFnSEfeJZWWuzsi7UBfz/U6n5RwSzrHVqnrGNJOWT6fLwCfJCXvC0s7ZQX3khrA\nv5CSz73A48A/A7tFxONK9wqUx2lvl+XhiOjpQKCkq/XfysaXzkr7kEhn4J/YqFDpbT2UW9omvVkG\nyNfgO5UBm8ZVpbpMoe9xRYXPVHM/jy7m0c6G7fAqG7ZDAG2ldaR0eepduS7tpDPmjdq4vP7K943y\ntqDWbVzn7UqF112NB3iE1Du3M717mt01EVHpxrzO9Tg1Iv5no0LTpY1+jy/gy6Te1RNJPVelNqi7\ndXwv8M+SLoqI17uYpruyryb1EC7OlyBKl6xnA58iHYyfDXyVlPBnlc2j1+1sv35NLh9ZrZK0l6Q6\noPxa052k7l8A1Lu7EPeQdFAePoF0llCuGVgDvCJpR+DIPH4JMFHSgbmsZkn1+b2ngA8D10h6cw/l\nf1jJ3qSu3WXlb0qaFBEPR8R3STvSPt3Maynpul7p+vqJpDOkko+WLee9ef4fkvTtHurYnduBk7Xh\n+ugu+exxFnCcpFGSmoF/6WlG+Qj4OVLX0VxSIH4l/++V0hF+TqCQYuIjuU6QuqK6uss9gCiLrSlA\nc16fd5LOvO4mref9ulnP55Aak9G5/rPyMpxGOsvtizWko29yuTtLOjgv4whJ+1dckHRT0XpJh+RR\n5T/z9r+ka4aStDvw9jx+DvCPpeWS1JQb/16XWyJpN0m392lJN9ZdXfaWtIdSVjuhl/Or5X5+P6nr\nFzbeDqtIPSONpDP295SVGZI+mOPwOGB87jVbDfwk16VO0kfoWa3buHKzyOsg94qtjIhXuxkPMI+0\n7/xG+Rslks6Q9Nk+lNvZ7cCppXZC6Rs4o3M9js/rdmfgH3s5v2MljcztzLtzncttBfw1nzx8kg0H\nMHeQ2s/RuR7l9zLMILU715e1Z5U0kmIE4GNs2L5NwLOSGvP4kvvycrXkA8VHSCdSvW5nyw3E9+DP\nIl03/B827io8Dfh7pZsmlpIqXbpR5CddzGsJ8BlJD5NWyIxO7y8gNSqLgZ+RE2M+ojoB+LGkh0gb\nquOMLyKWAp8AblTqWu7KclJQ/QY4pULvw1eUbsB5mLRz39HVjCLiNVJX8S8lPUI6CvtZ2SRjlG4e\n+xzpiBLSUfgr3dSvWxFxK/DfwP25zF8AYyPiAVJX90Ok7qOOo0Olr4FN62KWs0k7wut5eBf6HniX\nk7opr46IR0iJ+c68Dj9BuozQlXVsiK2nc9m/JF3L3wb4LGnnfYC0npeTDgR+1mk+pwMvk7o750bE\nM6RLKH1dliuBy5S6tIN03fP7OeYWkq47d+Uk4KeS7mPjXou7gWdIO/YFwCKAiHiOFD835PnPAfbO\n26Iv5UJa7s2+U7mburxG2ga3kdblk6XPDOL9/AvAF/O+Vx57L5GuT/+WdP231BX+OvB1UnfrK2y4\nPwJS8j1G0jpSrH4pjy/F5mBb9s7OA96Z6/ItUox2N75U1t2ks83f5iT4ZtJlic31U9LJ1CJJi0n3\nADSQ2rIVpHXxIzZut74j6Zgu5vcgKSbvA76R47fcj4Bpku4nXY56PS/XLaT2Zl7ex79Y/qGIuJC0\nba7KB3uVvAwcKGkBKVam5/Hnkdqp3+d5lOa5jtS7OCePmk26R6Bjmr7ws+gHAUkrgbdExOpO468j\n3SC2JTuLDXKSJgP/3blrfADLO5N0rfzWapQ3VOQzsRcjosdnJVjXJP0WeH8vr8kX1mCIJ/9c7CAW\nEb3t4jTrtYioyoNQbHiKiPfWug6WDPszeKXvlR7XafT1EdGbO9ltAEiax6YHnx/L3Y5DSu6W7vzM\nh+9HxNWVpreBMZz38+G87NUyWNusYZ/gzczMisg/NmNmZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQE\nb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZA\nTvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7wZmZm\nBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9m\nZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7w\nZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXk\nBG9mZlZATvBmZmYF5ARvZmZWQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZW\nQE7wZmZmBeQEb2ZmVkBO8GZmZgXkBG9mZlZATvBmZmYF5ARvZmZWQE7wZoOQpCskPS9pca3rYsXm\nWCsuJ3izwekq4KhaV8KGhatwrBWSE7zZIBQRs4CXal0PKz7HWnE5wZuZmRVQQ60rMNxsu+22MXHi\nxIrvSeoYjogu57FgwYIXI2K7fq+cDSmSTgFOAWhqanr7vvvuO+Blzp8/37E3DDnWhiYn+CqbOHEi\nc+fO3WR8Q0MD06dPZ9SoUbS2tnL22WfT2tpacR6NjY1PD3Q9bfCLiBnADIApU6bEvHnzBrxMSY69\nYcixNjQ5wQ8CDQ0NfOc73+HrX/96x7jW1lbOPffcLpO8mZlZd3wNvsbq6ur48pe/zDe+8Q0kccgh\nhyCJ888/n3POOYe6Om+i4UjSdcB9wD6SVko6udZ1smJyrBWXz+BrqL29nWXLlrFw4UIgXXffYYcd\nOq6/33PPPSxbtow999xzo+vzVnwRcUKt62DDg2OtuHx6WEMRwaOPPsq8efNob2/njDPO4H3vex9n\nnHEG7e3tLFy4kKVLl9Le3l7rqpqZ2RDjBF9Dkthtt92YPHkykrjkkkuYM2cOl1xyCZLYc889mThx\nos/ezcysz5zga2zKlCkcccQR1NXV0draymWXXdZxY93RRx/NgQceWOMampnZUORr8DXS3t7OiBEj\nABg3bhytra1st912jB49mpaWFp599lnGjRsHwIgRI2hpafENd2Zm1mvOGDUQEfzxj3/kC1/4An/9\n619pbW1l++2354UXXmDFihU8++yzbL311khi5cqVfPGLX+Tuu+/2tXgzM+s1n8HXQGNjI/fddx8/\n/OEPufjiixk5ciSnnnoqf/jDH1i/fj2jRo3iyCOPpLW1lZ133pmLLrqICRMmcPjhh/t78WZm1itO\n8DUyYsQIJCGJL33pSxx33HHMnj2bXXbZhZUrV9LU1MSvf/3rjmkaGrypzMys99xFXwNvvPEGEdGR\n5EeOHMmUKVM49dRTefrppznllFM46KCDaGxsRBJjxowhInjjjTdqXXUzMxsifFpYA5I455xzePHF\nF2lvb2fx4sUsW7aMmTNn0tLSwqWXXsqJJ57IYYcdxv777099fT3nnHOOu+eHGUlHARcD9cBlEXFB\njatkBeQ4Ky4n+BppbW3lwgsv7Oh6b2lpoa2tjbq6Otra2rjyyis77rJvbW11ch9mJNUDlwCHAyuB\nByXdHBFLa1szKxLHWbG5i77GSsm781fgSt+Ld2Iftg4GlkfEkxHRAlwPvL/GdbLicZwVmM/gq2zB\nggUv9sPPvVb+QXkrkp2BP5e9XglMLZ+g/De6gdclLa5CvfapQhlWPT3GGTjWhion+CqLiO1qXQcb\nEio9nzg2elH2G92S5kXElAGvlDTwPwRu1dRjnIFjbahyF73Z4LQS2LXs9S7AX2pUFysux1mBOcGb\nDU4PAntJ2kPSCOB44OYa18mKx3FWYO6iNxuEIqJV0unA7aSvL10REUu6+ciM6tSsauVYFWxGnIFj\nbchQxCaXW8zMzGyI67aLXtI2khblv2clPVP2esRAVEjSZEmLunjvHkkHbMY8p0m6qJfTzpR0bIXx\nUyX9oJvPHSbppr7WrS9yGS/n9f+opK9t4fymSzqzh2kqrjtJDZJWb2a5FdexmZn1n2676CPib8AB\nAJLOB9ZGxH+UTyNJpJ6AQv/UWUTMBebWuh7AXRFxrKSxwMOSbomIh0pvSmqICH953sxsmNusm+zy\nWfZiST8BFgC7lp/NSTpe0mV5eHtJv5Q0T9IDkt7RiyIaJf1c0iOSfiFpdIU6zMjzXCLpvLLxUyXd\nJ+khSXMljen0uX+VdK+kCd2Uf6Sk2ZIel3R0/lzHGbqk9+T5L5K0QFJT/ty4vKyPSbq6rMzD87SP\nSPpZqfdD0kpJF+T1MlfSnr1YNwBExFrSup+Uz7Kvl3QLcFue99l5vg93Wj/n5fr9Htirl8VNlHR7\n/ty5nd+U1CzpD3ldPCzpX8reOymPe0jSlRU++11Jl0uqGItdrSNJ78+vF0q6Q9Kb8vilksZJqpO0\nWtLH8vjrJP1TXlf/nZdnmaTv9nIdDFqSjsrbZrmksweojCskPa/qfP/ZBqFqxFkux7HWXyKiV3/A\n+cBX8vBkoB04KL9uAFaXTXs86ZnGADcA78jDuwOL8/BU4CcVyplM+h5m6TNXA2fm4XuAA/LwhLKy\nZwP7AaOAp4AD83tbkW4cmQZcBHwImAVs1c1yzgRuIR387EN6CMRI4DDgpjzNbcDUPDw2l3EYsArY\nMb9+EHgHMCbPY1Ke/hrg9Dy8EjgrD3+6NP9u6lZeh+2AFbmO04Cnga3ze8cAl5K+41oH/A54J+mp\nVQ8Bo/O6eaps3Z4GTKtQ5jTgGWBroAlYSurV6djmQCMwLg+/CViWh98K/KlsW00oW8fHAt8nPSZT\n3SxzxXWU61O6h+SzwPfy8GXAkbmODwI/zuOfyMs9DVgGjMuv/wzs1Nv9YLD95Vh7AtgTGJG3734D\nUM4/AAeS91//Da+/asVZLsux1k9/W3IX/RMR8WAvpjsM2Cf15AOwtaTR0X2X91MRcX8enkl6glLn\n68AnSDqZlGh2IiX4kcCKiFgAEBEvQ/pxF9Kzlg8Gjoh09tudX0S65PCYpD+z6ZnuvcBFkq4FboyI\ntbmM+yPir7nMRaQDmjdICe+J/NmrgZOBH+XX1+X/1wC9+ZGHQyUtJB1gfTsiHpP0buCOiFiVpzkC\nOBpYmF+PBfYGts31XQesk/Sb0kwj4pJuyry9NO/ci/EuoPzoWsD3JL0r12tXSdsC7wFuiIiXchkv\nlX3mm8CciPhcL5a50jraDfiFpB1I2/3xPH42qYF4jrSOPytpd+C5iFiXt9OdEbEmL8+f8ryG6nd/\nOx41CiCp9KjRfn2WeETMyuvRhqeqxBk41vrTlnwP/tWy4XY2fiLSqLJhAQdHxAH5b+ecYLrT+db+\njV5L2gs4A3hPRPwd6Qx1VC6rq68FLCedtfamW7rb8iNiOvDvpMT5YK4PwOtlk7WRDj4qPSmqu7J6\ncldEvC0i3h4RPysbX749BEwvW+eTI+KqzSyv0mc6v/430ro9MCIOAF6k5+3xADBF0tabUT6kM/8f\nRMT/AU5lQ8zNBt6d/+4CVpN6C2aVfbbSdhqqKj1qdOca1cWKy3E2BPXLg27y2e4qSXvla6kfKHv7\nTlL3LwDq3V3we0g6KA+fQOqaL9cMrAFekbQjqUsWYAnpevGBuaxmpV9LgtQd/WHgGklv7qH8DyvZ\nm/SUp2Xlb0qaFBEPR8R3SWfJ3T0zeSnpQRKl6+snAneXvf/RsuW8N8//Q5K+3UMdu3M7cHLp3gBJ\nu+Qz6lnAcZJGSWoG/qW7mZQ5QtJ4pfsZ3l+qZ5mtgOcjfaf2cDbs+HcCxyvf76CN73v4LfCfwC1K\nNwx2Z5N1lMt8RumU/JOlCfMZxk7AxIhYQYqdL5MSfxH16lGjZlvIcTYE9eeZy1mkM+kVpKQ2Mo8/\nDfixpJNRkeeXAAAGRklEQVRyeXcBp0maCpwUEZ+tMK8lwGckXU66htv5gQcLchmLgSfJjX5EvC7p\nhFzeKGAdqZuY/P5SSZ8AbpT03oh4qotlWU5Khm8CTomIlrJLDABfyd3i7cDDwB2kbuFNRMRr+VLC\nL/PBxlyg/Mx7jKQHSDvLCXncZOCVLurWo4i4VdK+wP253muAj0XEA5J+Rbp+9r+UndVKOg14PSIu\nqzDLe4BrgUnAzyNikaTy2Pk58BulZ0cvIB8QRcTDki4EZklqBeaTLk+U6nm9pHHAr/P2WN/FIlVa\nR+cDvyKdSTxAuveh5EHSpRFIif1bbHpQUhR+1KhVg+NsCPKDbmpI0krgLRGxutP460g34v2tNjUb\nPLpaR5bkA63HgX8m3Qz5IOlgrqenkW1OWbsDt0TEW/p73ja4VTPOcnm741jbYn4W/SAUESc4uVtv\nRHrmQelRo4+SbhAdiOR+HXAf6YbZlblXyoaJasUZONb607A9g1f6bvhxnUZfHxG9uZPd+pmkm0l3\ns5f7SkTcWYv6mJkNdcM2wZuZmRWZu+jNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3MzArICd7M\nzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAnezMysgJzg\nzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3MzArI\nCd7MzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAnezMys\ngJzgzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3M\nzArICd7MzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAne\nzMysgJzgzczMCsgJ3szMrICc4M3MzArICd7MzKyAnODNzMwKyAnezMysgJzgzczMCsgJ3szMrICc\n4M3MzArICd7MzKyAnODNzMwKyAnezMysgJzg+0jSFZKel7S41nWx4nKcWbU41orLCb7vrgKOqnUl\nrPCuwnFm1XEVjrVCcoLvo4iYBbxU63pYsTnOrFoca8XlBG9mZlZADbWuQBFJOgU4BaCpqent++67\n74CXOX/+/BcjYrsBL8gGDceZVYtjbWhygh8AETEDmAEwZcqUmDdv3oCXKenpAS/EBhXHmVWLY21o\nche9mZlZATnB95Gk64D7gH0krZR0cq3rZMXjOLNqcawVl7vo+ygiTqh1Haz4HGdWLY614vIZvJmZ\nWQE5wZuZmRWQE7yZmVkBOcGbmZkVkBO8mZlZATnBm5mZFZATvJmZWQE5wZuZmRWQE3wfSTpK0mOS\nlks6u9b1seJyrFk1OM6Kywm+DyTVA5cARwP7ASdI2q+2tbIicqxZNTjOis0Jvm8OBpZHxJMR0QJc\nD7y/xnWyYnKsWTU4zgrMCb5vdgb+XPZ6ZR5n1t8ca1YNjrMC84/N9I0qjItNJpJOAU7JL1+XtHhA\na5XsU4UyrHp6jDXHmfUDt2kF5gTfNyuBXcte7wL8pfNEETEDmAEgaV5ETBnoikmaN9BlWFX1GGuO\nM+sHbtMKzF30ffMgsJekPSSNAI4Hbq5xnayYHGtWDY6zAvMZfB9ERKuk04HbgXrgiohYUuNqWQE5\n1qwaHGfF5gTfRxFxK3BrHz4yY6DqUqNyrEr6GGuOM9ssbtOKSxGb3E9hZmZmQ5yvwZuZmRWQE/wA\nqdbjHyVdIen5Kn1txQahasSa48zcpg09TvADoMqPf7wKOGqA5m2DXBVj7SocZ8OW27ShyQl+YFTt\n8Y8RMQt4aSDmbUNCVWLNcTbsuU0bgpzgB4Yf/2jV4lizanCcDUFO8AOjV49/NOsHjjWrBsfZEOQE\nPzB69fhHs37gWLNqcJwNQU7wA8OPf7RqcaxZNTjOhiAn+AEQEa1A6fGPjwK/GKjHP0q6DrgP2EfS\nSkknD0Q5NjhVK9YcZ8Ob27ShyU+yMzMzKyCfwZuZmRWQE7yZmVkBOcGbmZkVkBO8mZlZATnBm5mZ\nFZATvJmZWQE5wZuZmRWQE7yZmVkB/X9yunLwaFCEQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01abb4c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import data_helper\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Convolutional layer 1\n",
    "filter_size_1 = 3\n",
    "num_filters_1 = 32\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter_size_2 = 3\n",
    "num_filters_2 = 64\n",
    "\n",
    "# Convolutional layer 3\n",
    "filter_size_3 = 3\n",
    "num_filters_3 = 64\n",
    "\n",
    "# Fully connected layer\n",
    "fc_size = 128   # Number of neurons in fully connected layer\n",
    "\n",
    "# Number of color channels in an image\n",
    "num_channels = 3    #RGB\n",
    "\n",
    "# Image dimensions\n",
    "img_size = 32\n",
    "\n",
    "# Size of image after flattening to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Class info\n",
    "\n",
    "classes = ['black_bishop', 'black_king', 'black_knight', 'black_pawn', 'black_queen', 'black_rook', 'blank', \\\n",
    "'white_bishop', 'white_king', 'white_knight', 'white_pawn', 'white_queen', 'white_rook']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Validation split\n",
    "validation_size = .2\n",
    "\n",
    "# How long to wait after validation loss stops improving before terminating training\n",
    "early_stopping = None  # use None if you don't want to implement early stoping\n",
    "\n",
    "train_path = 'train_data'\n",
    "test_path = 'test_data'\n",
    "\n",
    "data = data_helper.read_training_sets(train_path, img_size, classes, validation_size = validation_size)\n",
    "test_images, test_ids = data_helper.read_testing_set(test_path, img_size, classes)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "\n",
    "\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev = 0.05)) # Standard Deviation = 0.05\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape = [length]))\n",
    "\n",
    "\n",
    "def new_conv_layer(input,              # The previous layer\n",
    "               num_input_channels,     # Number of channels in previous layer\n",
    "               filter_size,            # Width and height of each filter\n",
    "               num_filters,            # Number of filters\n",
    "               use_pooling = True):    # Use 2x2 max-pooling\n",
    "\n",
    "    # Shape of the filter-weights for the convolution\n",
    "    # This format is determined by the TensorFlow API\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape\n",
    "    weights = new_weights(shape = shape)\n",
    "\n",
    "    # Create new biases, one for each filter\n",
    "    biases = new_biases(length = num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution\n",
    "    # Note the strides are set to 1 in all dimensions\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same\n",
    "    layer = tf.nn.conv2d(input = input, filter = weights, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution\n",
    "    # A bias-value is added to each filter-channel\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value = layer,\n",
    "                               ksize = [1, 2, 2, 1],       # Size of max-pooling window (2x2)\n",
    "                               strides = [1, 2, 2, 1],     # stride on a single image (2x2)\n",
    "                               padding = 'SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU)  (Some alien name as said by Siraj Raval :P)\n",
    "    # It calculates max(x, 0) for each input pixel x\n",
    "    # This adds some non-linearity to the formula and allows us to learn more complicated functions\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights\n",
    "\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,             # The previous layer\n",
    "                 num_inputs,        # Number of inputs from previous layer\n",
    "                 num_outputs,       # Number of outputs\n",
    "                 use_relu = True):  # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(length = num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = [None, img_size_flat], name = 'x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape = [None, num_classes], name = 'y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension = 1) # Returns the index with the largest value across axis of a tensor\n",
    "\n",
    "\n",
    "layer_conv1, weights_conv1 = \\\n",
    "new_conv_layer(input = x_image,\n",
    "               num_input_channels = num_channels,\n",
    "               filter_size = filter_size_1,\n",
    "               num_filters = num_filters_1,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "new_conv_layer(input = layer_conv1,\n",
    "               num_input_channels = num_filters_1,\n",
    "               filter_size = filter_size_2,\n",
    "               num_filters = num_filters_2,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "new_conv_layer(input = layer_conv2,\n",
    "               num_input_channels = num_filters_2,\n",
    "               filter_size = filter_size_3,\n",
    "               num_filters = num_filters_3,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = new_fc_layer(input = layer_flat,\n",
    "                         num_inputs = num_features,\n",
    "                         num_outputs = fc_size,\n",
    "                         use_relu = True)\n",
    "\n",
    "layer_fc2 = new_fc_layer(input = layer_fc1,\n",
    "                         num_inputs = fc_size,\n",
    "                         num_outputs = num_classes,\n",
    "                         use_relu = False)\n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2, name = 'y_pred')\n",
    "y_pred_cls = tf.argmax(y_pred, dimension = 1)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = layer_fc2, labels = y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "train_batch_size = batch_size\n",
    "\n",
    "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    acc = session.run(accuracy, feed_dict = feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict = feed_dict_validate)\n",
    "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "\n",
    "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        feed_dict_validate = {x: x_valid_batch, y_true: y_valid_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "\n",
    "\n",
    "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
    "        if i % int(data.train.num_examples/batch_size) == 0:\n",
    "            val_loss = session.run(cost, feed_dict = feed_dict_validate)\n",
    "            epoch = int(i / int(data.train.num_examples / batch_size))\n",
    "\n",
    "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
    "\n",
    "            if early_stopping:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                if patience == early_stopping:\n",
    "                    break\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(\"trained_model\"):\n",
    "        os.makedirs(\"trained_model\")\n",
    "    saver.save(session, 'trained_model/trained_model',global_step = 15000)\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred = None):\n",
    "\n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return\n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "\n",
    "\n",
    "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=5, wspace=5)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.valid.images[incorrect]\n",
    "\n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.valid.cls[incorrect]\n",
    "\n",
    "    # Plot the first 4 images.\n",
    "    plot_images(images = images[0:4],\n",
    "                cls_true = cls_true[0:4],\n",
    "                cls_pred = cls_pred[0:4])\n",
    "\n",
    "def print_validation_accuracy(show_example_errors = False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.valid.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape = num_test, dtype = np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.valid.images[i:j, :].reshape(batch_size, img_size_flat)\n",
    "\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.valid.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images, y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict = feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    cls_true = np.array(data.valid.cls)\n",
    "    cls_pred = np.array([classes[x] for x in cls_pred])\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred = cls_pred, correct = correct)\n",
    "\n",
    "\n",
    "\n",
    "optimize(num_iterations = 15000)\n",
    "print_validation_accuracy(show_example_errors = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
