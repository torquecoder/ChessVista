{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n",
      "Reading testing images\n",
      "Size of:\n",
      "- Training-set:\t\t564\n",
      "- Test-set:\t\t8\n",
      "- Validation-set:\t140\n",
      "Epoch 1 --- Training Accuracy:  25.0%, Validation Accuracy:   0.0%, Validation Loss: 2.519\n",
      "Epoch 2 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.782\n",
      "Epoch 3 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.654\n",
      "Epoch 4 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.968\n",
      "Epoch 5 --- Training Accuracy:  75.0%, Validation Accuracy:  25.0%, Validation Loss: 2.128\n",
      "Epoch 6 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.139\n",
      "Epoch 7 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.158\n",
      "Epoch 8 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.503\n",
      "Epoch 9 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 0.627\n",
      "Epoch 10 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.336\n",
      "Epoch 11 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 1.906\n",
      "Epoch 12 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%, Validation Loss: 0.784\n",
      "Epoch 13 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.093\n",
      "Epoch 14 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.005\n",
      "Epoch 15 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.080\n",
      "Epoch 16 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 0.378\n",
      "Epoch 17 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 1.518\n",
      "Epoch 18 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.154\n",
      "Epoch 19 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.278\n",
      "Epoch 20 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.099\n",
      "Epoch 21 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.244\n",
      "Epoch 22 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.209\n",
      "Epoch 23 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.127\n",
      "Epoch 24 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.202\n",
      "Epoch 25 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.048\n",
      "Epoch 26 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 27 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.077\n",
      "Epoch 28 --- Training Accuracy:  75.0%, Validation Accuracy: 100.0%, Validation Loss: 0.273\n",
      "Epoch 29 --- Training Accuracy:  75.0%, Validation Accuracy:  75.0%, Validation Loss: 0.230\n",
      "Epoch 30 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.200\n",
      "Epoch 31 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.026\n",
      "Epoch 32 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.274\n",
      "Epoch 33 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.005\n",
      "Epoch 34 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.014\n",
      "Epoch 35 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.617\n",
      "Epoch 36 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.143\n",
      "Epoch 37 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.105\n",
      "Epoch 38 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.640\n",
      "Epoch 39 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.596\n",
      "Epoch 40 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.278\n",
      "Epoch 41 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.057\n",
      "Epoch 42 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.019\n",
      "Epoch 43 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.044\n",
      "Epoch 44 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.005\n",
      "Epoch 45 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.009\n",
      "Epoch 46 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.436\n",
      "Epoch 47 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.012\n",
      "Epoch 48 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 49 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 50 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 51 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.041\n",
      "Epoch 52 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.422\n",
      "Epoch 53 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.003\n",
      "Epoch 54 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.027\n",
      "Epoch 55 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 56 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 57 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 58 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 59 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.037\n",
      "Epoch 60 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.028\n",
      "Epoch 61 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 62 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 63 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.198\n",
      "Epoch 64 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.007\n",
      "Epoch 65 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.075\n",
      "Epoch 66 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 67 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.426\n",
      "Epoch 68 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 69 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 70 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.226\n",
      "Epoch 71 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.033\n",
      "Epoch 72 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.039\n",
      "Epoch 73 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.837\n",
      "Epoch 74 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.164\n",
      "Epoch 75 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.222\n",
      "Epoch 76 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.015\n",
      "Epoch 77 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.087\n",
      "Epoch 78 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.020\n",
      "Epoch 79 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 80 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 81 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.510\n",
      "Epoch 82 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.002\n",
      "Epoch 83 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 84 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 85 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 86 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.028\n",
      "Epoch 87 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.073\n",
      "Epoch 88 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.012\n",
      "Epoch 90 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 91 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 92 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.042\n",
      "Epoch 93 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 94 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 95 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 96 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 97 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.004\n",
      "Epoch 98 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.006\n",
      "Epoch 99 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 100 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.001\n",
      "Epoch 101 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 102 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%, Validation Loss: 0.609\n",
      "Epoch 103 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 104 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.000\n",
      "Epoch 105 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.019\n",
      "Epoch 106 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.034\n",
      "Epoch 107 --- Training Accuracy: 100.0%, Validation Accuracy: 100.0%, Validation Loss: 0.107\n",
      "Time elapsed: 0:02:53\n",
      "Accuracy on Test-Set: 97.9% (137 / 140)\n",
      "Example errors:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-173777eaee31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-173777eaee31>\u001b[0m in \u001b[0;36mprint_validation_accuracy\u001b[0;34m(show_example_errors)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_example_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example errors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mplot_example_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-173777eaee31>\u001b[0m in \u001b[0;36mplot_example_errors\u001b[0;34m(cls_pred, correct)\u001b[0m\n\u001b[1;32m    366\u001b[0m     plot_images(images = images[0:4],\n\u001b[1;32m    367\u001b[0m                 \u001b[0mcls_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 cls_pred = cls_pred[0:4])\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_example_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-173777eaee31>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(images, cls_true, cls_pred)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Plot image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# Show true and predicted classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcYAAAD8CAYAAADt9ARWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhVFERCTy/wGzrw/9hElG5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01b04bba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import data_helper\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Convolutional layer 1\n",
    "filter_size_1 = 3\n",
    "num_filters_1 = 32\n",
    "\n",
    "# Convolutional layer 2\n",
    "filter_size_2 = 3\n",
    "num_filters_2 = 64\n",
    "\n",
    "# Convolutional layer 3\n",
    "filter_size_3 = 3\n",
    "num_filters_3 = 64\n",
    "\n",
    "# Fully connected layer\n",
    "fc_size = 128   # Number of neurons in fully connected layer\n",
    "\n",
    "# Number of color channels in an image\n",
    "num_channels = 3    #RGB\n",
    "\n",
    "# Image dimensions\n",
    "img_size = 32\n",
    "\n",
    "# Size of image after flattening to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Class info\n",
    "\n",
    "classes = ['black_bishop', 'black_king', 'black_knight', 'black_pawn', 'black_queen', 'black_rook', 'blank', \\\n",
    "'white_bishop', 'white_king', 'white_knight', 'white_pawn', 'white_queen', 'white_rook']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Validation split\n",
    "validation_size = .2\n",
    "\n",
    "# How long to wait after validation loss stops improving before terminating training\n",
    "early_stopping = None  # use None if you don't want to implement early stoping\n",
    "\n",
    "train_path = 'train_data'\n",
    "test_path = 'test_data'\n",
    "\n",
    "data = data_helper.read_training_sets(train_path, img_size, classes, validation_size = validation_size)\n",
    "test_images, test_ids = data_helper.read_testing_set(test_path, img_size, classes)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.valid.labels)))\n",
    "\n",
    "\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev = 0.05)) # Standard Deviation = 0.05\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape = [length]))\n",
    "\n",
    "\n",
    "def new_conv_layer(input,              # The previous layer\n",
    "               num_input_channels,     # Number of channels in previous layer\n",
    "               filter_size,            # Width and height of each filter\n",
    "               num_filters,            # Number of filters\n",
    "               use_pooling = True):    # Use 2x2 max-pooling\n",
    "\n",
    "    # Shape of the filter-weights for the convolution\n",
    "    # This format is determined by the TensorFlow API\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape\n",
    "    weights = new_weights(shape = shape)\n",
    "\n",
    "    # Create new biases, one for each filter\n",
    "    biases = new_biases(length = num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution\n",
    "    # Note the strides are set to 1 in all dimensions\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same\n",
    "    layer = tf.nn.conv2d(input = input, filter = weights, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution\n",
    "    # A bias-value is added to each filter-channel\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value = layer,\n",
    "                               ksize = [1, 2, 2, 1],       # Size of max-pooling window (2x2)\n",
    "                               strides = [1, 2, 2, 1],     # stride on a single image (2x2)\n",
    "                               padding = 'SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU)  (Some alien name as said by Siraj Raval :P)\n",
    "    # It calculates max(x, 0) for each input pixel x\n",
    "    # This adds some non-linearity to the formula and allows us to learn more complicated functions\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights\n",
    "\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,             # The previous layer\n",
    "                 num_inputs,        # Number of inputs from previous layer\n",
    "                 num_outputs,       # Number of outputs\n",
    "                 use_relu = True):  # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(length = num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = [None, img_size_flat], name = 'x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape = [None, num_classes], name = 'y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension = 1) # Returns the index with the largest value across axis of a tensor\n",
    "\n",
    "\n",
    "layer_conv1, weights_conv1 = \\\n",
    "new_conv_layer(input = x_image,\n",
    "               num_input_channels = num_channels,\n",
    "               filter_size = filter_size_1,\n",
    "               num_filters = num_filters_1,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv2, weights_conv2 = \\\n",
    "new_conv_layer(input = layer_conv1,\n",
    "               num_input_channels = num_filters_1,\n",
    "               filter_size = filter_size_2,\n",
    "               num_filters = num_filters_2,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_conv3, weights_conv3 = \\\n",
    "new_conv_layer(input = layer_conv2,\n",
    "               num_input_channels = num_filters_2,\n",
    "               filter_size = filter_size_3,\n",
    "               num_filters = num_filters_3,\n",
    "               use_pooling = True)\n",
    "\n",
    "layer_flat, num_features = flatten_layer(layer_conv3)\n",
    "\n",
    "layer_fc1 = new_fc_layer(input = layer_flat,\n",
    "                         num_inputs = num_features,\n",
    "                         num_outputs = fc_size,\n",
    "                         use_relu = True)\n",
    "\n",
    "layer_fc2 = new_fc_layer(input = layer_fc1,\n",
    "                         num_inputs = fc_size,\n",
    "                         num_outputs = num_classes,\n",
    "                         use_relu = False)\n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc2)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension = 1)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = layer_fc2, labels = y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "train_batch_size = batch_size\n",
    "\n",
    "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
    "    # Calculate the accuracy on the training-set.\n",
    "    acc = session.run(accuracy, feed_dict = feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict = feed_dict_validate)\n",
    "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))\n",
    "\n",
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
    "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
    "\n",
    "        # Convert shape from [num examples, rows, columns, depth]\n",
    "        # to [num examples, flattened image shape]\n",
    "\n",
    "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
    "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        feed_dict_validate = {x: x_valid_batch, y_true: y_valid_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "\n",
    "\n",
    "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
    "        if i % int(data.train.num_examples/batch_size) == 0:\n",
    "            val_loss = session.run(cost, feed_dict = feed_dict_validate)\n",
    "            epoch = int(i / int(data.train.num_examples / batch_size))\n",
    "\n",
    "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
    "\n",
    "            if early_stopping:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                if patience == early_stopping:\n",
    "                    break\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(\"trained_model\"):\n",
    "        os.makedirs(\"trained_model\")\n",
    "    saver.save(session, 'trained_model/trained_model',global_step = 15000)\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred = None):\n",
    "\n",
    "    if len(images) == 0:\n",
    "        print(\"no images to show\")\n",
    "        return\n",
    "    else:\n",
    "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
    "\n",
    "\n",
    "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=5, wspace=5)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_example_errors(cls_pred, correct):\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.valid.images[incorrect]\n",
    "\n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.valid.cls[incorrect]\n",
    "\n",
    "    # Plot the first 4 images.\n",
    "    plot_images(images = images[0:4],\n",
    "                cls_true = cls_true[0:4],\n",
    "                cls_pred = cls_pred[0:4])\n",
    "\n",
    "def print_validation_accuracy(show_example_errors = False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.valid.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape = num_test, dtype = np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.valid.images[i:j, :].reshape(batch_size, img_size_flat)\n",
    "\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.valid.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images, y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict = feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    cls_true = np.array(data.valid.cls)\n",
    "    cls_pred = np.array([classes[x] for x in cls_pred])\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred = cls_pred, correct = correct)\n",
    "\n",
    "\n",
    "\n",
    "optimize(num_iterations = 15000)\n",
    "print_validation_accuracy(show_example_errors = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
